---
title: "Hepatitic C Prediction Dataset"
author:
- name: Stephen Hepper
date: '`r format(Sys.Date(), "%B %e, %Y")`'
output:
    prettydoc::html_pretty:
       theme: cayman
       highlight: github
editor_options:
    chunk_output_type: console
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  warning = FALSE,
  message = FALSE
)
```

```{r eval = FALSE}
Data: https://www.kaggle.com/fedesoriano/hepatitis-c-dataset
```

```{r}
library(faux)
library(kableExtra) 
library(car)
library(ggplot2)
library(glmnet)
library(readxl)
library(genefilter)
library(tidyr)
library(MASS)
library(plyr)
library(dplyr)
library(pROC)
set.seed(111)
```

```{r}

data = data.frame(read.csv("HepatitisCdata.csv"))
data_test = data
data_test = data_test %>% drop_na()
```


```{r}

data_test$Category = as.factor(data_test$Category)

data_test$Sex = as.factor(data_test$Sex)
data_test = data_test %>% drop_na()

data_test$Sex = revalue(data_test$Sex, c("m"=0))
data_test$Sex = revalue(data_test$Sex, c("f"=1))

metabolites = (colnames(data_test))[5:14]

fm = as.formula(paste0("Sex ~", 
          paste0(metabolites, collapse = "+")))


```

# Logistic Regression - Prediction of Sex from Metabolite Concentrations
```{r}
fit.glm = glm(fm, 
               data= data_test,
               family=binomial(link="logit"))
```

```{r}
OR <- exp(coef(fit.glm))
kable(OR, digits =3, col.names = c("Odds ratio estimate"))
```

```{r, echo = FALSE}
predicted_sex = predict(fit.glm,type='response')
```

```{r}
actual = data_test$Sex

df.pred = data.frame(actual= data_test$Sex,
                 predicted = predicted_sex,
                 class_0.5 = ifelse(predicted_sex <= 0.5, "pred -", "pred +"),
                 class_0.3 = ifelse(predicted_sex <= 0.3, "pred -", "pred +"))
df_0 = subset(df.pred, actual == 0)
df_1 = subset(df.pred, actual == 1)
kable(rbind(df_0[1:3,], df_1[1:3,]), 
      col.names = c("Response", "Predicted probability", "Class 0.5 cutoff", "Class 0.3 cutoff"),
      digits = 3)

xtab5 = table(df.pred$class_0.5, df.pred$actual)
xtab3 = table(df.pred$class_0.3, df.pred$actual)

xtab5               # confusion matrix for threshold of 0.5

xtab3               # confusion matrix for threshold of 0.3
```

```{r}
auc = roc(df.pred$actual,predicted_sex)
p.roc = ggroc(auc, color = "darkred") 
```

```{r, out.height = '75%', echo = FALSE}
p.roc = p.roc +
      annotate("text", x = 0.25, y = 0.75, label = paste("AUC =", round(auc$auc[1],3)),
               color = "darkred", size = 6)+
  theme_bw()+  theme(axis.title.x = element_text( size=12), 
                          axis.text.x  = element_text( size=12),
                          axis.text.y  = element_text( size=12),
                          axis.title.y=element_text(size=12))
p.roc
```

# K-Nearest Neighbors - Hepatitis Category Classification

```{r}
library(caret)
library(class)

knn_data = data
knn_data = knn_data %>% drop_na()

knn_data$Category = as.factor(knn_data$Category)
#knn_data$Sex = as.factor(data_test$Sex)

pred_vars  = (colnames(knn_data))[3:14]

knn_data$Sex = revalue(knn_data$Sex, c("m"=0))
knn_data$Sex = revalue(knn_data$Sex, c("f"=1))

split_index = createDataPartition(data$Category, 
  p = .75, #the percentage of data that goes to training
  list = FALSE)#should the results be in a list (TRUE) or a matrix
knn_data_train = knn_data[ split_index, ]
knn_data_test = knn_data[-split_index, ]

knn_data_train = knn_data_train %>% drop_na()
knn_data_test = knn_data_test %>% drop_na()

#knn_res <- knn(knn_data_train[,pred_vars], knn_data_test[,pred_vars], cl = knn_data_train$Category, 
   # k = 5,  prob = TRUE)
```

```{r}
trControl = trainControl(method  = "cv",
                          number  = 5)#for # of folds
```

### KNN - Fit model on the training data
```{r}

knn_fit = train(Category ~ .,
    method     = "knn",
    tuneGrid   = expand.grid(k = 1:10),
    trControl  = trControl,
    metric     = "Accuracy",
    data       = knn_data_train[,c("Category", pred_vars)])
```

```{r}
knn_fit
```

```{r}
knn_pred = predict(knn_fit, 
                    newdata= knn_data_test[,c("Category", pred_vars)])
knn_data_test$predicted = knn_pred
```

###  KNN - prediction with 5 folds, first
```{r}

knn_res = knn(knn_data_train[,pred_vars], knn_data_test[,pred_vars], cl = knn_data_train$Category, 
    k = 5,  prob = TRUE)

knn_data_test$actual = factor(knn_data_test$Category)
knn_data_test$prob = attributes(knn_res)$prob
table(knn_data_test$actual, knn_data_test$predicted)

sum(diag(table(knn_data_test$actual, knn_data_test$predicted)))/
  sum(table(knn_data_test$actual, knn_data_test$predicted))

```

### KNN - prediction with 1 fold after cross-validation 
```{r}

knn_res = knn(knn_data_train[,pred_vars], knn_data_test[,pred_vars], cl = knn_data_train$Category, 
    k = 1,  prob = TRUE)

knn_data_test$predicted = knn_res

knn_data_test$actual = factor(knn_data_test$Category)
knn_data_test$prob = attributes(knn_res)$prob
table(knn_data_test$actual, knn_data_test$predicted)

sum(diag(table(knn_data_test$actual, knn_data_test$predicted)))/
  sum(table(knn_data_test$actual, knn_data_test$predicted))
```


# Regression Analysis (Ridge and Lasso)

```{r}
reg_data = data
reg_data = reg_data %>% drop_na()
reg_data$Age = as.double(reg_data$Age)

reg_fm = as.formula(paste0("Age ~", 
          paste0(metabolites, collapse = "+")))

```

## Ridge

```{r}
library(glmnet)
x=model.matrix(reg_fm,data)[,-1]
y = reg_data$Age


grid=10^seq(10,-2,length=100)#fit the model: Ridge Regression
ridge.mod=glmnet(x,y,alpha=0,lambda=grid)
```

```{r}
cv.fit = cv.glmnet(x,y,alpha=0, lambda=grid)
plot(cv.fit)

cv.fit$lambda.min
coef(ridge.mod, s = cv.fit$lambda.min)

ridge.fitted <-predict(cv.fit, newx = x, s = "lambda.min")

# Mean Squared Error
mean((ridge.fitted-y)^2)
```

## Lasso 

```{r}
lasso.mod=glmnet(x,y,alpha=1,lambda=grid)

cv.fit = cv.glmnet(x,y,alpha=1, lambda=grid)
plot(cv.fit)

cv.fit$lambda.min
coef(lasso.mod, s = cv.fit$lambda.min)

lasso.fitted = predict(cv.fit, newx = x, s = "lambda.min")

# Mean Squared Error
mean((lasso.fitted-y)^2)

```

# PCA Analysis of Metabolites

```{r}
pca_data = data_test[,metabolites]
pca_data = na.omit(pca_data)

res.pca = prcomp(pca_data, scale = TRUE)

```

```{r}
library(factoextra)

ev = res.pca$sdev^2
#percentage of variance explained by the first 10 components
per_var = 100*cumsum(ev/sum(ev))[1:10]
per_var

# Scree plot elbow looks to be at dimension/variable 6
fviz_eig(res.pca)   
```

```{r}
eig.val = get_eigenvalue(res.pca)
head(eig.val)

pca_contribution = data.frame(round(res.pca$rotation[1:10,c(1:2)], 2))

# I created a custom "significance" threshold of having <-0.2 or 0.2> on BOTH PC1 and 
# PC2. Only 5 variables meet or exceed this threshold. If the "AND" operator is changed to 
# "OR", then 9 variables meet or exceed the threshold
sig_vars = pca_contribution[ which( abs(pca_contribution$PC1) >= 0.2 & 
                                      abs(pca_contribution$PC2) >=0.2 ), ]

sig_var_names = rownames(sig_vars)

```

```{r}
fviz_pca_var(res.pca, col.var = "contrib",
             gradient.cols =c("red","white","blue"),ggtheme =theme_minimal())

#GGT (Gamma-Glutamyl Transferase), and CHE (Acetylcholinesterase) 
#look to have the greatest contribution, with ALB (Albumin Blood Test), CHOL (Cholesterol), 
#ALP (Alkaline phosphatase), and AST (Aspartate Transaminase) close behind
# CHE and CHOL have a hint of correlation, as do GGt and ALP possibly

#Interestingly CREA (Creatine) has the lowest contribution, not BIL (Bilurbin),
# Which was considered to be the least important variable by the lasso regression
```
